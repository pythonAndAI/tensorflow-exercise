卷积神经网络--->Convolutional Neural Network，CNN

1.卷积神经网络的应用非常广泛，在自然语言处理、医药发现、灾难气候发现甚至围棋人工智能程序都有应用。

2.卷积神经网络中的著名数据集和模型。
2.1 数据集
2.1.1 MNIST数据集：是一个手写体识别数据集，图片的大小为28*28，主要分为0-9的数字，下载地址为http://yann.lecun.com/exdb/mnist/
2.1.2 CIFAR(CIFAR-10和CIFAR-100):是一个图像分类数据集，它们都是图像词典项目中800万张图片的一个子集，CIFAR数据集中的图片大小为32*32，所以放大之后图片比较模糊，CIFAR官网https://www.cs.toronto.edu/~kriz/cifar.html
提供了不同格式的CIFAR数据集下载。CIFAR-10收集了来自10个不同种类的60000张图片，图片大小都是固定的且每一张图片中仅包含一个种类的实体，和MNIST数据集最大的区别是图片由黑白变成了彩色，且分类难度也相对更高。
在CIFAR-10数据集上，人工标注的正确率大概为94%。
2.1.3 ImageNet数据集：是一个基于WordNet的大型图像数据库。在ImageNet中，将近1500万图片被关联到了WordNet的大约20000个名词同义词集上。目前每一个与ImageNet相关的WordNet同义词集都代表了现实世界中的一个实体，可以被
认为是分类问题中的一个别类。ImageNet中的图片都是从互联网上爬取下来的，并且通过亚马逊的人工标注服务将图片分类到WordNet的同义词集上。在ImageNet的图片中，一张图片中可能出现多个同义词集所代表的实体。ImageNet官网为
http://image-net.org/challenges/LSVRC/2012/。ImageNet图像分类数据集上的top-5正确率。top-N正确率指的是图像识别算法给出前N个答案中有一个是正确的概率。
2.2 模型
2.2.1 在ImageNet数据集上面模型的正确率：LeNet-v4-->ResNet-->Inception v3-->LeNet-->VGG-->ZF-->AlexNet

3.卷积神经网络简介
3.1. 在全连接神经网络中，每相邻两层之间的节点都有边相连，于是一般会将每一层全连接层中的节点组织成一列，这样方便显示连接结构。而对于卷积神经网络，相邻两层之间只有部分节点相连，为了展示每一层神经元的维度，一般会将
每一层卷积层的节点组织成一个三维矩阵。所以全连接和卷积的结构相似。
3.2 卷积神经网络的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度。卷积神经网络和全连接神经网络的唯一区别就在于神经网络中相邻两层的连接方式。
3.3 全连接的弊端：使用全连接神经网络的最大问题在于全连接层的参数太多。对于MNIST数据，每一张图片的大小是28x28x1，其中28x28为图片的大小，x1表示图片是黑白的，只有一个色彩通道。假设第一层的隐藏层的节点数为500个，那么
一个全连接层的神经网络将有28x28x500+500=392500个参数。当图片更大时，比如在CIFAR-10数据集中，图片的大小为32x32x3，其中32x32表示图片的大小，x3表示图片是通过红绿蓝三个色彩通道表示的。这样输入层就有3072个节点，如果第一
层全连接层仍然是500个节点，那么这一层全连接神经网络将有32x32x3x500+500=150万个参数。参数增多除了导致计算速度减慢，还很容易导致过拟合问题。所以需要一个更合理的神经网络结构来有效地减少神经网络中的参数个数。
卷积神经网络就可以达到这个目的。
3.4 卷积神经网络架构图：输入层--->卷积层1--->池化层1--->卷积层2--->池化层2--->全连接层1--->全连接层2--->Softmax--->输出层。在卷积神经网络的前几层中，每一层的节点都被组织成一个三维矩阵。比如处理CIFAR-10数据集中的图片时，
可以将输入层组织成一个32x32x3的三维矩阵。并且卷积神经网络中前几层中每一个节点只和上一层中部分的节点相连。
3.5 组成。
3.5.1 输入层：输入层是整个神经网络的输入，在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵。比如3.4的结构，最左侧的三维矩阵就可以代表一张图片。其中三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了
图像的色彩通道，比如黑白图片的深度为1，而在RGB色彩模式下，图像的深度为3.从输入层开始，卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵，直到最后的全连接层。
3.5.2 卷积层：卷积层的每一个节点的输入只是上一层神经网络的一小块，这个小块常用的大小为3x3或者5x5。卷积层试图将神经网络中的每一小块进行更深入地分析从而得到抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵会
变得更深，所以在3.4可以看到经过卷积层之后的节点矩阵的深度会增加。
3.5.3 池化层(Pooling)：池化层神经网路不会改变三维矩阵的深度，但是它可以缩小矩阵的大小。池化操作可以认为是将一张分辨率高较高的图片转化为分辨率较低的图片，通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到
减少整个神经网络中参数的目的。
3.5.4 全连接层：在经过多轮卷积层和池化层的处理之后，在卷积神经网络的最后一般会是由1到2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以
将卷积层和池化层看成自动图像特征提取的过程，在特征提取完成之后，仍然需要使用全连接层来完成分类任务。
3.5.5 Softmax层：Softmax层主要用于分类问题，通过Softmax层，可以得到当前样例属于不同种类的概率分布情况。

4.卷积层
4.1 卷积层神经网络最主要部分为过滤器(filter)或者内核(kernel)。过滤器可以将当前层神经网络上的一个子节点矩阵转化为下一层神经网络上的一个单位节点矩阵。单位节点矩阵指的是一个长和宽都为1，但深度不限的节点矩阵。
4.2 在一个卷积层中，过滤器所处理的节点矩阵的长和宽都是由人工指定的，这个节点矩阵的尺寸也被称之为过滤器的尺寸。常用的过滤器尺寸有3x3或者5x5。因为过滤器处理的矩阵深度和当前层神经网络节点矩阵的深度是一致的，
所以虽然节点矩阵是三维的，但过滤器的尺寸只需要指定两个维度。过滤器中另外一个需要人工指定的设置是处理得到的单位节点矩阵的深度，这个设置称为过滤器的深度。注意过滤器的尺寸指的是一个过滤器输入节点矩阵的大小，
而深度指的是输出单位节点矩阵的深度。
4.3 卷积层结构的前向传播过程就是通过将一个过滤器从神经网络当前层的左上角移动到右下角，并且在移动中计算每一个对应的单位矩阵得到的。
4.4 当过滤器的大小不为1x1时，卷积层前向传播得到的矩阵的尺寸要小于当前层矩阵的尺寸。比如，当前层矩阵的大小为3x3，而通过卷积层前向传播算法之后，得到的矩阵大小为2x2，为了避免尺寸的变化，可以在当前层矩阵的边界上加入全0填充。
这样可以使得卷积层前向传播结果矩阵的大小和当前层矩阵保持一致。
4.5 除了使用全0填充，还可以通过设置过滤器移动的步长来调整结果矩阵的大小。比如长和宽的步长均为2时，过滤器每隔2步计算一次结果，所以得到的结果矩阵的长和宽也就都只有原来的一般。那么一个4x4的矩阵得到的前向传播结果矩阵为2x2。
4.6 在卷积神经网络中，每一个卷积层中使用的过滤器中的参数都是一样的。这是卷积神经网络一个非常重要的性质。从直观上理解，共享过滤器的参数可以使得图像上的内容不受位置的影响。
4.7 共享每一个卷积层中过滤器中的参数可以巨幅减少神经网络上的参数。以CIFAR-10问题为例，输入层矩阵的维度是32x32x3,。假设第一层卷积层使用尺寸为5x5，深度为16的过滤器，那么这个卷积层的参数个数为5x5x3x16+16=1216个。前面提到过，
使用500个隐藏节点的全连接层将有1.5百万个参数。相比之下，卷积层的参数个数要远远小于全连接层。而且卷积层的参数个数和图片的大小无关，它只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关，这使得卷积神经网络可以很好地扩展到
更大的图像数据上。

5.池化层
5.1 在卷积层之间往往会加上一个池化层。池化层可以非常有效地缩小矩阵的尺寸(长和宽)，从而减少最后全连接层中的参数。使用池化层既可以加快计算速度也可以防止过拟合问题的作用。
5.2 和卷积层类似，池化层的前向传播过程也是通过一个类似过滤器的结构完成的。不过池化层过滤器中的计算不是节点的加权和，而是采用更加简单的最大值或者平均值运算。使用最大值操作的池化层被称为最大池化层(max pooling),
这是被使用最多的池化层结构。使用平均值操作的池化层被称之为平均池化层(average pooling)。
5.3 与卷积层的过滤器类似，池化层的过滤器也需要人工设定过滤器的尺寸、是否使用全0填充以及过滤器移动的步长等设置，而且这些设置的意义也是一样的。卷积层和池化层中过滤器移动的方式是相似的，唯一的区别在于卷积层使用的
过滤器是横跨整个深度的，而池化层使用的过滤器只影响一个深度上的节点。所以池化层的过滤器除了在长和宽两个维度移动，它还需要在深度这个维度移动。

6.卷积层计算和取值
6.1 当使用全0填充时，卷积层的输出矩阵大小为：out(length)=[in(length) / stride(length)] ，其中out(length)表示输出层矩阵的长度。它等于输入层矩阵长度除以长度方向上的步长的向上取整值。
类似的out(width)=[in(width) / stride(width)]，其中out(width)表示输出层矩阵的宽度。它等于输入层矩阵宽度除以宽度方向上的步长的向上取整值。
6.2 当不使用全0填充时，卷积层的输出矩阵大小为：out(length)=[(in(length) - filter(length) +1) / stride(length)]，其中out(length)表示输出层矩阵的长度。它等于输入层矩阵长度减去长度上的过滤器地尺寸加上1除以长度方向上的步长的向上取整值。
类似的out(width)=[(in(width) - filter(width) +1) / stride(width)]，其中out(width)表示输出层矩阵的宽度。它等于输入层矩阵宽度减去宽度上的过滤器地尺寸加上1除以宽度方向上的步长的向上取整值。

7.如何设计卷积神经网络的架构。
7.1 以下正则表达式公式总结了一些经典的用于图片分类问题的卷积神经网络架构：输入层-->（卷积层+-->池化层？ ）+-->全连接层+
7.2 在过滤器深度上，大部分卷积神经网络都采用逐层递增的方式。比如每经过一次池化层之后，卷积层过滤器的深度都会乘以2.虽然不同的模型会选择使用不同的具体数字，但是逐层递增是比较普遍的模式。
卷积层的步长一般为1，但是也有可能是2或3作为步长，卷积层的尺寸一般为3*3或5*5。池化层比较简单，一般都是最大池化层，池化层的过滤器边长一般为2或者3，步长一般为2或者3

8.LeNet模型结构
8.1 第一层卷积层：输入为32*32*1的矩阵，过滤器尺寸为5*5，深度为6，不使用全0填充，步长为1。卷积层输入大小为28*28*6，一共有5*5*6+6=156个参数，共有28*28*6*(25+1)=122304个连接
8.2 第一层池化层：输入为28*28*6，池化层步长为2，过滤器尺寸为2*2，所以本层的输出矩阵大小为14*14*6
8.3 第二层卷积层：输入为14*14*6的矩阵，过滤器尺寸为5*5，深度为16，不使用全0填充，步长为1。卷积层输入大小为10*10*16，一共有5*5*16+16=2416个参数，共有10*10*16*(25+1)=41600个连接
8.4 第二层池化层：输入为10*10*16，池化层步长为2，过滤器尺寸为2*2，所以本层的输出矩阵大小为5*5*16
8.5 第一层全连接层：输入为5*5*16，拉伸成一个[batch, 5*5*16]的二维矩阵，输出节点120个，总共有5*5*16*120+120=48120个参数，输出为一个[batch, 120]的二维矩阵
8.6 第二层全连接层：输入为[batch, 120]的二维矩阵，输出节点84个，总共有120*84+84=10164个参数，输出为一个[batch, 84]的二维矩阵
8.7 第三层全连接层：输入为[batch, 84]的二维矩阵，输出节点10个，总共有84*10+10=850个参数，输出为一个[batch, 10]的二维矩阵
