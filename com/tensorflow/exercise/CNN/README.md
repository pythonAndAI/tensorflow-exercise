卷积神经网络--->Convolutional Neural Network，CNN

1.卷积神经网络的应用非常广泛，在自然语言处理、医药发现、灾难气候发现甚至围棋人工智能程序都有应用。

2.卷积神经网络中的著名数据集和模型。
2.1 数据集
2.1.1 MNIST数据集：是一个手写体识别数据集，图片的大小为28*28，主要分为0-9的数字，下载地址为http://yann.lecun.com/exdb/mnist/
2.1.2 CIFAR(CIFAR-10和CIFAR-100):是一个图像分类数据集，它们都是图像词典项目中800万张图片的一个子集，CIFAR数据集中的图片大小为32*32，所以放大之后图片比较模糊，CIFAR官网https://www.cs.toronto.edu/~kriz/cifar.html
提供了不同格式的CIFAR数据集下载。CIFAR-10收集了来自10个不同种类的60000张图片，图片大小都是固定的且每一张图片中仅包含一个种类的实体，和MNIST数据集最大的区别是图片由黑白变成了彩色，且分类难度也相对更高。
在CIFAR-10数据集上，人工标注的正确率大概为94%。
2.1.3 ImageNet数据集：是一个基于WordNet的大型图像数据库。在ImageNet中，将近1500万图片被关联到了WordNet的大约20000个名词同义词集上。目前每一个与ImageNet相关的WordNet同义词集都代表了现实世界中的一个实体，可以被
认为是分类问题中的一个别类。ImageNet中的图片都是从互联网上爬取下来的，并且通过亚马逊的人工标注服务将图片分类到WordNet的同义词集上。在ImageNet的图片中，一张图片中可能出现多个同义词集所代表的实体。ImageNet官网为
http://image-net.org/challenges/LSVRC/2012/。ImageNet图像分类数据集上的top-5正确率。top-N正确率指的是图像识别算法给出前N个答案中有一个是正确的概率。
2.2 模型
2.2.1 在ImageNet数据集上面模型的正确率：LeNet-v4-->ResNet-->Inception v3-->LeNet-->VGG-->ZF-->AlexNet

3.卷积神经网络简介
3.1. 在全连接神经网络中，每相邻两层之间的节点都有边相连，于是一般会将每一层全连接层中的节点组织成一列，这样方便显示连接结构。而对于卷积神经网络，相邻两层之间只有部分节点相连，为了展示每一层神经元的维度，一般会将
每一层卷积层的节点组织成一个三维矩阵。所以全连接和卷积的结构相似。
3.2 卷积神经网络的输入层就是图像的原始像素，而输出层中的每一个节点代表了不同类别的可信度。卷积神经网络和全连接神经网络的唯一区别就在于神经网络中相邻两层的连接方式。
3.3 全连接的弊端：使用全连接神经网络的最大问题在于全连接层的参数太多。对于MNIST数据，每一张图片的大小是28x28x1，其中28x28为图片的大小，x1表示图片是黑白的，只有一个色彩通道。假设第一层的隐藏层的节点数为500个，那么
一个全连接层的神经网络将有28x28x500+500=392500个参数。当图片更大时，比如在CIFAR-10数据集中，图片的大小为32x32x3，其中32x32表示图片的大小，x3表示图片是通过红绿蓝三个色彩通道表示的。这样输入层就有3072个节点，如果第一
层全连接层仍然是500个节点，那么这一层全连接神经网络将有32x32x3x500+500=150万个参数。参数增多除了导致计算速度减慢，还很容易导致过拟合问题。所以需要一个更合理的神经网络结构来有效地减少神经网络中的参数个数。
卷积神经网络就可以达到这个目的。
3.4 卷积神经网络架构图：输入层--->卷积层1--->池化层1--->卷积层2--->池化层2--->全连接层1--->全连接层2--->Softmax--->输出层。在卷积神经网络的前几层中，每一层的节点都被组织成一个三维矩阵。比如处理CIFAR-10数据集中的图片时，
可以将输入层组织成一个32x32x3的三维矩阵。并且卷积神经网络中前几层中每一个节点只和上一层中部分的节点相连。
3.5 组成。
3.5.1 输入层：输入层是整个神经网络的输入，在处理图像的卷积神经网络中，它一般代表了一张图片的像素矩阵。比如3.4的结构，最左侧的三维矩阵就可以代表一张图片。其中三维矩阵的长和宽代表了图像的大小，而三维矩阵的深度代表了
图像的色彩通道，比如黑白图片的深度为1，而在RGB色彩模式下，图像的深度为3.从输入层开始，卷积神经网络通过不同的神经网络结构将上一层的三维矩阵转化为下一层的三维矩阵，直到最后的全连接层。
3.5.2 卷积层：卷积层的每一个节点的输入只是上一层神经网络的一小块，这个小块常用的大小为3x3或者5x5。卷积层试图将神经网络中的每一小块进行更深入地分析从而得到抽象程度更高的特征。一般来说，通过卷积层处理过的节点矩阵会
变得更深，所以在3.4可以看到经过卷积层之后的节点矩阵的深度会增加。
3.5.3 池化层(Pooling)：池化层神经网路不会改变三维矩阵的深度，但是它可以缩小矩阵的大小。池化操作可以认为是将一张分辨率高较高的图片转化为分辨率较低的图片，通过池化层，可以进一步缩小最后全连接层中节点的个数，从而达到
减少整个神经网络中参数的目的。
3.5.4 全连接层：在经过多轮卷积层和池化层的处理之后，在卷积神经网络的最后一般会是由1到2个全连接层来给出最后的分类结果。经过几轮卷积层和池化层的处理之后，可以认为图像中的信息已经被抽象成了信息含量更高的特征。我们可以
将卷积层和池化层看成自动图像特征提取的过程，在特征提取完成之后，仍然需要使用全连接层来完成分类任务。
3.5.5 Softmax层：Softmax层主要用于分类问题，通过Softmax层，可以得到当前样例属于不同种类的概率分布情况。

4.卷积层
4.1 卷积层神经网络最主要部分为过滤器(filter)或者内核(kernel)。过滤器可以将当前层神经网络上的一个子节点矩阵转化为下一层神经网络上的一个单位节点矩阵。单位节点矩阵指的是一个长和宽都为1，但深度不限的节点矩阵。
4.2 在一个卷积层中，过滤器所处理的节点矩阵的长和宽都是由人工指定的，这个节点矩阵的尺寸也被称之为过滤器的尺寸。常用的过滤器尺寸有3x3或者5x5。因为过滤器处理的矩阵深度和当前层神经网络节点矩阵的深度是一致的，
所以虽然节点矩阵是三维的，但过滤器的尺寸只需要指定两个维度。过滤器中另外一个需要人工指定的设置是处理得到的单位节点矩阵的深度，这个设置称为过滤器的深度。注意过滤器的尺寸指的是一个过滤器输入节点矩阵的大小，
而深度指的是输出单位节点矩阵的深度。
4.3 卷积层结构的前向传播过程就是通过将一个过滤器从神经网络当前层的左上角移动到右下角，并且在移动中计算每一个对应的单位矩阵得到的。
4.4 当过滤器的大小不为1x1时，卷积层前向传播得到的矩阵的尺寸要小于当前层矩阵的尺寸。比如，当前层矩阵的大小为3x3，而通过卷积层前向传播算法之后，得到的矩阵大小为2x2，为了避免尺寸的变化，可以在当前层矩阵的边界上加入全0填充。
这样可以使得卷积层前向传播结果矩阵的大小和当前层矩阵保持一致。
4.5 除了使用全0填充，还可以通过设置过滤器移动的步长来调整结果矩阵的大小。比如长和宽的步长均为2时，过滤器每隔2步计算一次结果，所以得到的结果矩阵的长和宽也就都只有原来的一般。那么一个4x4的矩阵得到的前向传播结果矩阵为2x2。
4.6 在卷积神经网络中，每一个卷积层中使用的过滤器中的参数都是一样的。这是卷积神经网络一个非常重要的性质。从直观上理解，共享过滤器的参数可以使得图像上的内容不受位置的影响。
4.7 共享每一个卷积层中过滤器中的参数可以巨幅减少神经网络上的参数。以CIFAR-10问题为例，输入层矩阵的维度是32x32x3,。假设第一层卷积层使用尺寸为5x5，深度为16的过滤器，那么这个卷积层的参数个数为5x5x3x16+16=1216个。前面提到过，
使用500个隐藏节点的全连接层将有1.5百万个参数。相比之下，卷积层的参数个数要远远小于全连接层。而且卷积层的参数个数和图片的大小无关，它只和过滤器的尺寸、深度以及当前层节点矩阵的深度有关，这使得卷积神经网络可以很好地扩展到
更大的图像数据上。

